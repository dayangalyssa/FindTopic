# ğŸ“Œ FindTopic

## ğŸ“– Deskripsi Proyek
 **Proyek ini dirancang untuk mengotomatisasi pengumpulan dan pembersihan data penelitian menggunakan pendekatan MLOps, sehingga mempermudah analisis dan penerapan topic modeling secara efisien!** ğŸ“šğŸ“‹

## ğŸ“‚ Struktur Direktori
```
mlops-scrapping/
â”‚â”€â”€ data/                 # Folder untuk menyimpan dataset
â”‚   â”œâ”€â”€ research_titles.json  # Hasil scraping
â”‚   â”œâ”€â”€ processed_articles.json  # Data yang bersih setelah preprocessing
â”‚â”€â”€ script/                 # Folder untuk menyimpan dataset
â”‚   â”œâ”€â”€ embeddings.py        # Script untuk embedding dengan TF-IDF
â”‚   â”œâ”€â”€ explore_data.py      # Script untuk EDA dan Preprocessing
â”‚   â”œâ”€â”€ scrape.py            # Script untuk scraping
â”‚â”€â”€ models/               # Model hasil training
â”‚â”€â”€ notebooks/            # Notebook eksplorasi data
â”‚â”€â”€ scrap/                # Folder tambahan untuk proses scraping
â”‚â”€â”€ venv/                 # Virtual environment
â”‚â”€â”€ .gitignore            # File untuk mengabaikan file tertentu di Git
â”‚â”€â”€ README.md             # Dokumentasi proyek
â”‚â”€â”€ requirements.txt      # Dependencies yang dibutuhkan
```

## ğŸ”§ Cara Instalasi
1. **Clone repositori ini:**
   ```bash
   git clone https://github.com/dayangalyssa/mlops-scrapping.git
   ```
2. **Masuk ke direktori proyek:**
   ```bash
   cd mlops-scrapping
   ```
3. **Buat environment virtual (opsional tetapi disarankan):**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Mac/Linux
   venv\Scripts\activate     # Windows
   ```
4. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ’» Cara Menjalankan
### **1ï¸âƒ£ Scraping Data**
Untuk menjalankan scraping dan menyimpan hasilnya di `data/research_titles.json`, jalankan:
```bash
python data/scrape.py
```
### **2ï¸âƒ£ Membersihkan Data**
Untuk membersihkan data yang sudah di-scrape dan menyimpannya di `data/processed_articles.json`, jalankan:
```bash
python data/explore_data.py
```

## ğŸ› ï¸ Teknologi yang Digunakan
- **Python** ğŸ â†’ Bahasa utama dalam pengolahan data dan otomasi workflow
- **Scholarly** ğŸ” â†’ Untuk scraping publikasi akademik dari Google Scholar
- **pandas** ğŸ“Š â†’ Untuk manipulasi dan analisis data
- **spaCy** ğŸ“ â†’ Untuk pembersihan teks seperti lemmatization dan stopword removal
- **MLOps workflow** âš™ï¸ â†’ Untuk otomatisasi pipeline

## ğŸ“„ Penjelasan
### **`scrape.py` (Scraping Judul Penelitian)**
- Melakukan pencarian publikasi akademik menggunakan Scholarly.
- Mengambil judul, tahun, penulis, dan URL publikasi.
- Menyimpan hasil dalam `data/research_titles.json`.

### **`clean_data.py` (Pembersihan Data)**
- Membaca data dari `data/research_titles.csv`.
- Menggunakan spaCy untuk lemmatization dan stopword removal.
- Menyimpan hasil bersih ke `data/processed_articles.json`.

## ğŸ“Š Hasil
âœ… **Hasil scraping** disimpan dalam **`data/research_titles.json`**.  
âœ… **Hasil preprocessing** tersimpan dalam **`data/processed_articles.json`**.  